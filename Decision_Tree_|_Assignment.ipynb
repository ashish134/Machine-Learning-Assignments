{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLHUNJ8UDPSF+lkU4DzyM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish134/Machine-Learning-Assignments/blob/main/Decision_Tree_%7C_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree | Assignment\n",
        " **Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?**\n",
        "Answer:\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm that predicts an output by following a series of decision rules based on the values of input features. In classification, it‚Äôs used to assign data points into predefined categories.\n",
        "\n",
        "How it works in classification:\n",
        "\n",
        "1. Root node creation ‚Äì The algorithm starts with the entire dataset at the root node.\n",
        "\n",
        "2. Best split selection ‚Äì At each node, it evaluates all features and possible split points using an impurity measure such as Gini Impurity or Entropy. The split that results in the most ‚Äúpure‚Äù child nodes is chosen.\n",
        "\n",
        "3. Recursive splitting ‚Äì The dataset is divided into smaller subsets, and this process is repeated for each subset, creating branches of the tree.\n",
        "\n",
        "4. Stopping criteria ‚Äì Splitting stops when a node becomes pure (all samples from one class) or when other conditions are met (e.g., maximum depth, minimum samples per node).\n",
        "\n",
        "5. Prediction ‚Äì To classify a new observation, the model starts at the root and follows the decision rules until it reaches a leaf node. The majority class in that leaf is the prediction.\n",
        "\n",
        "Example: If petal length ‚â§ 2.45 cm ‚Üí classify as Setosa. Else if petal width ‚â§ 1.75 cm ‚Üí classify as Versicolor. Otherwise ‚Üí classify as Virginica.\n",
        "\n",
        "Advantages: simple to understand, interpretable, handles numeric & categorical data.\n",
        "\n",
        "Limitation: prone to overfitting if not properly controlled.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jn8fEuQvu8WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "Answer-\n",
        "Gini Impurity measures how often a randomly chosen sample would be incorrectly classified if it were labeled according to the distribution of classes in the node.\n",
        "\n",
        "Formula\n",
        "                Gini=1‚àíi=1‚àëC‚Äãpi2‚Äã\n",
        "Where:\n",
        "pùëñ = = probability of class i in the node\n",
        "C = number of classes\n",
        "nterpretation\n",
        "\n",
        "Gini = 0 ‚Üí Pure node (only one class)\n",
        "\n",
        "Higher Gini ‚Üí more impurity\n",
        "\n",
        "When used?\n",
        "\n",
        "Default in CART (Classification and Regression Trees)\n",
        "\n",
        "2. Entropy (Information Gain)\n",
        "Definition\n",
        "\n",
        "Entropy measures the uncertainty or randomness in a node.\n",
        "\n",
        "Formula         Entropy=‚àíi=1‚àëC‚Äãpi‚Äãlog2‚Äã(pi‚Äã)\n",
        "Interpretation\n",
        "\n",
        "Entropy = 0 ‚Üí Completely pure node\n",
        "\n",
        "Higher entropy ‚Üí more impurity / randomness\n",
        "Used with:\n",
        "\n",
        "Information Gain ‚Äì which tells how much entropy is reduced after a split.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZvzUY8lsvRdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "Pre-Pruning (Early Stopping)\n",
        "*   The tree‚Äôs growth is stopped early based on certain conditions before it becomes overly complex.\n",
        "*   Common stopping criteria:\n",
        "    *   Maximum depth (max_depth)\n",
        "    *   Minimum samples to split (min_samples_split)\n",
        "    *   Minimum samples in a leaf (min_samples_leaf)\n",
        "    *   Minimum impurity decrease\n",
        "*   Practical advantage: Saves computation time and prevents overfitting by keeping the model simpler from the start.\n",
        "\n",
        "Post-Pruning (Prune After Full Growth)\n",
        "*   The tree is grown to its maximum size (or nearly so), then branches that provide little predictive power are removed.\n",
        "\n",
        "*   Techniques:\n",
        "    *  Reduced Error Pruning (evaluate on validation set and remove unhelpful branches)\n",
        "    *  Cost Complexity Pruning (ccp_alpha in scikit-learn)\n",
        "*   Practical advantage: Allows the model to initially capture complex patterns and then simplifies it for better generalization, often leading to improved accuracy on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ecia8jfLwWER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "Answer:\n",
        "  Information Gain is a metric used in Decision Trees to measure how much uncertainty (entropy) is reduced after splitting a dataset using a particular feature.\n",
        "It tells us how much more ‚Äúinformation‚Äù the split provides.\n",
        "Information Gain Formula\n",
        "          Information Gain=Entropy(parent)‚àíi=1‚àëk‚ÄãNNi‚Äã‚Äã‚ãÖEntropy(childi‚Äã)\n",
        "  N = total samples in the parent node\n",
        "  Ni= samples in each child node\n",
        "  k = number of child nodes\n",
        "  Entropy measures impurity\n",
        "Why it‚Äôs important for choosing the best split:\n",
        "*   At each node, the Decision Tree algorithm evaluates all possible splits and computes their Information Gain.\n",
        "*   Higher Information Gain means the split makes the child nodes purer (more homogeneous) compared to the parent.\n",
        "*   The split with the maximum Information Gain is chosen because it best separates the classes, leading to a more accurate and efficient tree.\n",
        "Example: If splitting on ‚Äúpetal length ‚â§ 2.45‚Äù reduces entropy from 1.0 to an average of 0.2 in the child nodes, the Information Gain is 0.8, meaning the split significantly improves class purity.\n"
      ],
      "metadata": {
        "id": "l3CZECHTyptl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "Answer:\n",
        "\n",
        "Common real-world applications:\n",
        "\n",
        "  1. Medical diagnosis ‚Äì Predicting if a patient has a disease based on symptoms and test results.\n",
        "  2. Credit scoring ‚Äì Classifying loan applicants as ‚Äúlow risk‚Äù or ‚Äúhigh risk‚Äù based on financial history.\n",
        "  3. Fraud detection ‚Äì Identifying suspicious transactions.\n",
        "  4. Customer churn prediction ‚Äì Predicting if a customer will stop using a service.\n",
        "  5. Product recommendation ‚Äì Suggesting products based on user behavior and preferences.\n",
        "  6. Regression tasks ‚Äì Predicting house prices, sales forecasting, etc.\n",
        "Main advantages:\n",
        "\n",
        "*   Easy to understand & interpret ‚Äì Produces clear, human-readable rules.\n",
        "*   Handles both numerical and categorical data ‚Äì Works with mixed data types without complex preprocessing.\n",
        "*   No need for feature scaling ‚Äì Normalization or standardization is not required.\n",
        "*  Captures non-linear relationships ‚Äì Can model complex decision boundaries.\n",
        "*  Feature importance ‚Äì Identifies which variables are most influential.\n",
        "\n",
        "Main limitations:\n",
        "\n",
        "  *   Overfitting ‚Äì Fully grown trees can memorize the training data and perform poorly on unseen data.\n",
        "  *   High variance ‚Äì Small changes in data can produce very different trees.\n",
        "  *   Bias toward features with many levels ‚Äì Features with more unique values can dominate splits.\n",
        "  *   Less accurate than ensembles ‚Äì Often outperformed by Random Forests or Gradient Boosting on complex problems.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0uq2b-re0PON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL QUESTIONS**\n",
        "6. Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "‚óè Print the model‚Äôs accuracy and feature importances"
      ],
      "metadata": {
        "id": "_kyVUJDR12ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split into training and test sets (stratified to maintain class proportions)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Output\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Feature importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"  {name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub8F4uX_1wlB",
        "outputId": "b4c6656e-7f96-4b88-99fb-f37f4f08b136"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333\n",
            "Feature importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0286\n",
            "  petal length (cm): 0.5412\n",
            "  petal width (cm): 0.4303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "kRsTH_5U2byW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train/test sets (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Fully-grown tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "# Tree with max_depth=3\n",
        "clf_md3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_md3.fit(X_train, y_train)\n",
        "acc_md3 = accuracy_score(y_test, clf_md3.predict(X_test))\n",
        "\n",
        "# Output\n",
        "print(f\"Fully-grown tree accuracy: {acc_full:.4f}\")\n",
        "print(f\"Max depth=3 tree accuracy: {acc_md3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En55xLTQ2VCD",
        "outputId": "6a3bbc40-39a6-4735-eb81-2becbb3c7775"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 0.9333\n",
            "Max depth=3 tree accuracy: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to:\n",
        "#‚óè Load the California Housing dataset from sklearn\n",
        "\n",
        "#‚óè Train a Decision Tree Regressor\n",
        "\n",
        "#‚óè Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "#Answer:\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error on test data: {mse:.4f}\\n\")\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\" - {name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmlclaQL2ixc",
        "outputId": "cd8deeb7-dcce-49e4-cc37-c0018a8532cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on test data: 0.4952\n",
            "\n",
            "Feature Importances:\n",
            " - MedInc: 0.5285\n",
            " - HouseAge: 0.0519\n",
            " - AveRooms: 0.0530\n",
            " - AveBedrms: 0.0287\n",
            " - Population: 0.0305\n",
            " - AveOccup: 0.1308\n",
            " - Latitude: 0.0937\n",
            " - Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to:\n",
        "#‚óè Load the Iris Dataset\n",
        "\n",
        "#‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using GridSearchCV\n",
        "\n",
        "#‚óè Print the best parameters and the resulting model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up the grid of parameters to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearch to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Evaluate the best estimator on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Model accuracy on test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diblQoQa2wH_",
        "outputId": "8422cff5-ec0f-46c9-f26b-b9fd73062852"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model accuracy on test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you‚Äôre working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "‚óè Handle the missing values\n",
        "\n",
        "‚óè Encode the categorical features\n",
        "\n",
        "‚óè Train a Decision Tree model\n",
        "\n",
        "‚óè Tune its hyperparameters\n",
        "\n",
        "‚óè Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Handle Missing Values\n",
        "\n",
        "*   Understand the missingness: First, analyze how data is missing. Is it random, or does it follow a pattern? This affects how you handle it\n",
        "*   Imputation:\n",
        "  *   For numerical features, you could fill missing values with mean, median, or use more advanced methods like K-Nearest Neighbors imputation.\n",
        "  *   For categorical features, you might fill missing with the mode (most frequent value) or create a special category like \"Unknown\".\n",
        "  *   If missing values are too prevalent or critical, consider dropping those features or samples carefully.\n",
        "*   Why it matters: Models can‚Äôt handle missing data directly, so cleaning this up ensures your model sees complete, reliable inputs.\n",
        "\n",
        "2. Encode Categorical Features\n",
        "  *   Identify categorical variables: This could be patient gender, blood type, or any non-numeric info.\n",
        "  *   Encoding methods:\n",
        "      *   For nominal categories without order (e.g., blood type), use One-Hot Encoding.\n",
        "      *   For ordinal categories (e.g., disease severity: mild, moderate, severe), use Label Encoding or map them to meaningful numeric scales\n",
        "*   Why it matters: Machine learning models, including Decision Trees, require numeric input, so encoding transforms your data into a digestible form.\n",
        "*   3. Train a Decision Tree Model\n",
        "      *   Split the data: Use an 80-20 or 70-30 split between training and test sets, or use cross-validation to ensure your results generalize.\n",
        "      *   Initialize the model: Start with a default Decision Tree classifier.\n",
        "      *   Train on processed data: Fit the model on your training data.\n",
        "      *   Why Decision Trees: They handle mixed data types well, are interpretable (important in healthcare), and can capture nonlinear patterns.\n",
        "4. Tune Hyperparameters\n",
        "*   Key hyperparameters to tune:\n",
        "    *   max_depth: controls tree complexity, balancing underfitting and overfitting.\n",
        "    *   min_samples_split and min_samples_leaf: control how many samples needed to split or be a leaf node, affecting generalization.\n",
        "    *   max_features: number of features to consider at each split.\n",
        "*   Use GridSearchCV or RandomizedSearchCV: Explore combinations systematically with cross-validation to find the sweet spot.\n",
        "*   Why tuning matters: Proper tuning prevents overfitting or underfitting, improving the model‚Äôs predictive power on unseen data.\n",
        "\n",
        "5. Evaluate Performance\n",
        "\n",
        "*   Metrics:\n",
        "  *   For classification, consider Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "  *   In healthcare, Recall (sensitivity) is often critical ‚Äî you want to catch as many patients with the disease as possible, even at the cost of some false positives.\n",
        "\n",
        "*   Validation: Use a separate test set or cross-validation to ensure your model performs reliably.\n",
        "\n",
        "*  Interpretability: Use feature importance and decision tree visualization to explain model decisions to clinicians and stakeholders.\n",
        "\n",
        "**Business Value of This Model**\n",
        "\n",
        "*   Early detection: Predicting disease early means patients can receive timely treatment, improving outcomes and reducing healthcare costs.\n",
        "*   Resource optimization: Helps healthcare providers prioritize high-risk patients for screening or intervention, making better use of limited resources.\n",
        "*   Personalized care: Tailors monitoring and care plans based on individual risk, improving patient satisfaction and effectiveness.\n",
        "*   Data-driven decisions: Provides actionable insights backed by data, enabling the company to develop better products, policies, or outreach programs.\n",
        "\n",
        "*   Trust and transparency: Decision Trees‚Äô interpretability supports building trust with clinicians, regulators, and patients, crucial in healthcare.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EY5DT5FH3UzL"
      }
    }
  ]
}